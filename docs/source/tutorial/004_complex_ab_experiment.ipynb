{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a65a88a",
   "metadata": {},
   "source": [
    "# EarlySign Tutorial 004: Complex A/B Experiment with Guardrails and Adaptive Information Time\n",
    "\n",
    "This tutorial demonstrates a realistic A/B testing scenario combining:\n",
    "\n",
    "1. **Primary endpoint**: CTR improvement (A vs B) with group sequential testing (5 looks, both futility and efficacy)\n",
    "2. **Guardrail metrics**: Two safety metrics monitored via safe testing\n",
    "3. **Adaptive information time**: Re-estimation based on observed sample sizes\n",
    "4. **Multiple testing correction**: Bonferroni allocation (5% for A/B, 5% total for guardrails)\n",
    "5. **Wall-clock scheduling**: Progress reports every 3 days over 2-week period\n",
    "\n",
    "## Business Context\n",
    "\n",
    "**Scenario**: We want to improve webpage CTR by testing a new design (B) against baseline (A).\n",
    "- **Primary metric**: Click-through rate (higher is better, so B > A is the alternative)\n",
    "- **Guardrail 1**: Page load time (B should not significantly increase load time)\n",
    "- **Guardrail 2**: Bounce rate (B should not significantly increase bounce rate)\n",
    "\n",
    "**Statistical Design**:\n",
    "- Primary: Two-sided GST with O'Brien-Fleming spending, Î±=0.05, 5 looks\n",
    "- Guardrails: One-sided safe tests, Î±=0.025 each (Bonferroni: 0.05/2)\n",
    "- Information time: Adaptive based on daily sample size variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6dfa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework components now available in earlysign.api.ab_test\n",
    "from earlysign.api.ab_test import (\n",
    "    ab_test_with_guardrails,\n",
    "    GuardrailConfig,\n",
    "    ABTestExperiment,\n",
    ")\n",
    "from earlysign.methods.group_sequential.adaptive import (\n",
    "    AdaptiveInfoTime,\n",
    "    AdaptiveGSTBoundary,\n",
    ")\n",
    "\n",
    "# Runtime components\n",
    "from earlysign.runtime import SequentialRunner\n",
    "\n",
    "# Other imports\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from earlysign.backends.polars.ledger import PolarsLedger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aebc4e",
   "metadata": {},
   "source": [
    "## Problem: Multiple Statistics in Same Namespace\n",
    "\n",
    "Before building the experiment, let's highlight the current design issue:\n",
    "\n",
    "**Current Issue**: When multiple statistics are used in the same experiment (e.g., CTR Wald Z, Load Time Safe Test, Bounce Rate Safe Test), they all go to `namespace='stats'`. The `latest(namespace='stats')` call becomes ambiguous.\n",
    "\n",
    "**Current Workaround**: Use `tag` parameter: `latest(namespace='stats', tag='stat:ctr_waldz')`\n",
    "\n",
    "**Proposed Solutions**:\n",
    "1. Add a `metric_id` field for explicit metric identification\n",
    "2. Use hierarchical `statistic_type` field: `stats.ctr.waldz`, `stats.loadtime.safetest`\n",
    "3. Enhanced `tag` system with required tags for statistics\n",
    "\n",
    "For this demo, we'll use the current `tag`-based approach but highlight where the new design would be cleaner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e16ab5",
   "metadata": {},
   "source": [
    "## Experiment Design: Multi-Metric A/B Test Module\n",
    "\n",
    "We'll create a custom module that coordinates multiple statistics and criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90178b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Union\n",
    "from earlysign.core.ledger import Ledger\n",
    "\n",
    "# Import the framework components we just created\n",
    "from earlysign.api.multi_metric import MultiMetricABTest, GuardrailConfig\n",
    "from earlysign.methods.group_sequential.adaptive import AdaptiveInfoTime\n",
    "\n",
    "# Example usage with the new framework components\n",
    "print(\"âœ… Now using framework components for:\")\n",
    "print(\"  - AdaptiveInfoTime: earlysign.methods.group_sequential.adaptive\")\n",
    "print(\"  - MultiMetricABTest: earlysign.api.multi_metric\")\n",
    "print(\"  - GuardrailConfig: earlysign.api.multi_metric\")\n",
    "\n",
    "# Demo the AdaptiveInfoTime component\n",
    "adaptive_demo = AdaptiveInfoTime(initial_target=1000, looks=5)\n",
    "print(f\"\\nðŸ“Š AdaptiveInfoTime Demo:\")\n",
    "print(f\"  Planned fractions: {adaptive_demo.planned_fractions}\")\n",
    "\n",
    "# At look 3, observed 800 samples instead of planned 600\n",
    "adapted_t = adaptive_demo.get_info_fraction(current_look=3, observed_n=800)\n",
    "print(\n",
    "    f\"  Look 3 - Planned: {adaptive_demo.planned_fractions[2]:.3f}, Adapted: {adapted_t:.3f}\"\n",
    ")\n",
    "\n",
    "# Create a multi-metric experiment using the framework\n",
    "guardrails = [\n",
    "    GuardrailConfig(name=\"loadtime\", alpha=0.025, method=\"safe_test\"),\n",
    "    GuardrailConfig(name=\"bounce\", alpha=0.025, method=\"safe_test\"),\n",
    "]\n",
    "\n",
    "print(f\"\\nðŸ”§ Framework-based Multi-Metric Experiment:\")\n",
    "print(f\"  Components moved to: earlysign.api.multi_metric\")\n",
    "print(f\"  Adaptive info time moved to: earlysign.methods.group_sequential.adaptive\")\n",
    "print(f\"  Ready for reuse across projects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1602816",
   "metadata": {},
   "source": [
    "## Data Simulation: Multi-Metric Observations\n",
    "\n",
    "We'll simulate realistic daily data with:\n",
    "- Variable daily sample sizes\n",
    "- Correlated metrics (users with different characteristics)\n",
    "- True effects reflecting business scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Configure Multi-Metric A/B Test\n",
    "\n",
    "# Configure guardrails using domain-friendly API\n",
    "guardrails = [\n",
    "    GuardrailConfig(name=\"loadtime\", alpha=0.025, method=\"safe_test\"),\n",
    "    GuardrailConfig(name=\"bounce\", alpha=0.025, method=\"safe_test\"),\n",
    "]\n",
    "\n",
    "# Create comprehensive A/B test experiment\n",
    "experiment = ab_test_with_guardrails(\n",
    "    experiment_id=\"exp#web_test\",\n",
    "    primary_alpha=0.05,  # Full alpha for conversion rate\n",
    "    guardrails=guardrails,  # Safety monitoring\n",
    "    looks=5,\n",
    "    adaptive_info=True,  # Adapt to real sample sizes\n",
    "    target_n_per_arm=1000,\n",
    ")\n",
    "\n",
    "print(f\"âœ… A/B test configured with {len(guardrails)} guardrails\")\n",
    "print(f\"Primary endpoint: Î± = {experiment.primary_alpha}\")\n",
    "print(f\"Guardrail metrics: {[g.name for g in experiment.guardrails]}\")\n",
    "print(f\"Adaptive information timing: {experiment.adaptive_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730273a",
   "metadata": {},
   "source": [
    "## Running the Multi-Metric Experiment\n",
    "\n",
    "Now we'll run the 2-week experiment with progress reports every 3 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae4592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment setup\n",
    "exp_id = \"exp#web_ctr_improvement\"\n",
    "experiment = MultiMetricABTest(\n",
    "    experiment_id=exp_id,\n",
    "    primary_alpha=0.05,\n",
    "    guardrail_alpha_total=0.05,\n",
    "    looks=5,\n",
    "    spending=\"obf\",\n",
    "    target_n_per_arm=1400,  # Target for 2 weeks\n",
    ")\n",
    "\n",
    "runner = SequentialRunner(experiment, PolarsLedger())\n",
    "print(f\"ðŸš€ Experiment initialized: {exp_id}\")\n",
    "print(f\"   Components: {len(experiment.components)}\")\n",
    "print(f\"   Target sample size: {experiment.target_n_per_arm} per arm\")\n",
    "\n",
    "# Simulation parameters\n",
    "experiment_days = 14\n",
    "report_interval = 3  # Every 3 days\n",
    "report_days = [\n",
    "    report_interval * i for i in range(1, experiment_days // report_interval + 1)\n",
    "]\n",
    "if experiment_days not in report_days:\n",
    "    report_days.append(experiment_days)\n",
    "\n",
    "print(f\"ðŸ“… Schedule: {experiment_days} days, reports on days {report_days}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8570c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment day by day\n",
    "cumulative_data = {\n",
    "    \"ctr\": {\"a_success\": 0, \"a_total\": 0, \"b_success\": 0, \"b_total\": 0},\n",
    "    \"loadtime\": {\"a_success\": 0, \"a_total\": 0, \"b_success\": 0, \"b_total\": 0},\n",
    "    \"bounce\": {\"a_success\": 0, \"a_total\": 0, \"b_success\": 0, \"b_total\": 0},\n",
    "}\n",
    "\n",
    "daily_results = []\n",
    "look_counter = 0\n",
    "stopped_early = False\n",
    "stop_reason = None\n",
    "\n",
    "for day in range(1, experiment_days + 1):\n",
    "    # Simulate daily data\n",
    "    day_data = simulator.simulate_day(day, rng)\n",
    "\n",
    "    # Accumulate data\n",
    "    for metric in [\"ctr\", \"loadtime\", \"bounce\"]:\n",
    "        cumulative_data[metric][\"a_success\"] += day_data[metric][\"a_successes\"]\n",
    "        cumulative_data[metric][\"a_total\"] += day_data[metric][\"a_total\"]\n",
    "        cumulative_data[metric][\"b_success\"] += day_data[metric][\"b_successes\"]\n",
    "        cumulative_data[metric][\"b_total\"] += day_data[metric][\"b_total\"]\n",
    "\n",
    "    daily_results.append(\n",
    "        {\n",
    "            \"day\": day,\n",
    "            \"total_n\": cumulative_data[\"ctr\"][\"a_total\"]\n",
    "            + cumulative_data[\"ctr\"][\"b_total\"],\n",
    "            \"ctr_a_rate\": cumulative_data[\"ctr\"][\"a_success\"]\n",
    "            / max(1, cumulative_data[\"ctr\"][\"a_total\"]),\n",
    "            \"ctr_b_rate\": cumulative_data[\"ctr\"][\"b_success\"]\n",
    "            / max(1, cumulative_data[\"ctr\"][\"b_total\"]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Progress report and analysis on scheduled days\n",
    "    if day in report_days and not stopped_early:\n",
    "        look_counter += 1\n",
    "        time_index = f\"day_{day:02d}\"\n",
    "        step_key = f\"look_{look_counter}\"\n",
    "\n",
    "        print(f\"\\nðŸ“Š === Day {day} Analysis (Look {look_counter}) ===\")\n",
    "        print(f\"Cumulative sample: {daily_results[-1]['total_n']} total\")\n",
    "\n",
    "        # Add observations to ledger for each metric\n",
    "        # Note: This highlights the namespace/tag issue - we need different tags for each metric\n",
    "\n",
    "        for metric_name, data in cumulative_data.items():\n",
    "            # Each metric gets its own observation event with distinct tag\n",
    "            runner.ledger.write_event(\n",
    "                time_index=time_index,\n",
    "                namespace=Namespace.OBS,\n",
    "                kind=\"batch\",\n",
    "                experiment_id=exp_id,\n",
    "                step_key=step_key,\n",
    "                payload_type=\"TwoPropObsBatch\",\n",
    "                payload={\n",
    "                    \"nA\": data[\"a_total\"],\n",
    "                    \"mA\": data[\"a_success\"],\n",
    "                    \"nB\": data[\"b_total\"],\n",
    "                    \"mB\": data[\"b_success\"],\n",
    "                },\n",
    "                tag=f\"obs:{metric_name}\",  # Distinct tag per metric\n",
    "            )\n",
    "\n",
    "        # Run analysis step\n",
    "        experiment.step(runner.ledger, exp_id, step_key, time_index)\n",
    "\n",
    "        # Check for early stopping signals\n",
    "        signals = []\n",
    "        for tag in [\"crit:ctr_gst\", \"crit:loadtime_safe\", \"crit:bounce_safe\"]:\n",
    "            latest_signal = runner.ledger.latest(\n",
    "                namespace=Namespace.SIGNALS, tag=f\"{tag.replace('crit:', '')}:decision\"\n",
    "            )\n",
    "            if latest_signal:\n",
    "                signals.append(latest_signal)\n",
    "\n",
    "        # Report current metrics\n",
    "        for metric_name in [\"ctr\", \"loadtime\", \"bounce\"]:\n",
    "            data = cumulative_data[metric_name]\n",
    "            a_rate = data[\"a_success\"] / max(1, data[\"a_total\"])\n",
    "            b_rate = data[\"b_success\"] / max(1, data[\"b_total\"])\n",
    "            effect = b_rate - a_rate\n",
    "            print(\n",
    "                f\"  {metric_name.upper():>8s}: A={a_rate:.3f}, B={b_rate:.3f}, Effect={effect:+.3f}\"\n",
    "            )\n",
    "\n",
    "        # Check stopping conditions\n",
    "        if signals:\n",
    "            print(f\"  ðŸ”” {len(signals)} signal(s) detected\")\n",
    "            for signal in signals:\n",
    "                if signal.payload.get(\"action\") == \"stop\":\n",
    "                    stopped_early = True\n",
    "                    stop_reason = f\"Day {day}: {signal.tag}\"\n",
    "                    print(f\"  ðŸ›‘ Early stop triggered: {signal.tag}\")\n",
    "\n",
    "        if not stopped_early:\n",
    "            print(f\"  âœ… Continue to next look\")\n",
    "\n",
    "print(f\"\\nðŸ === Experiment Complete ===\")\n",
    "if stopped_early:\n",
    "    print(f\"Stopped early: {stop_reason}\")\n",
    "else:\n",
    "    print(f\"Completed full {experiment_days}-day period\")\n",
    "\n",
    "print(f\"Final sample size: {daily_results[-1]['total_n']}\")\n",
    "print(f\"Total looks executed: {look_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b0531",
   "metadata": {},
   "source": [
    "## Analysis: Ledger Inspection and Multiple Statistics Issue\n",
    "\n",
    "Let's examine the ledger to see how multiple statistics in the same namespace create ambiguity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4f5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the ledger structure\n",
    "ledger = runner.ledger\n",
    "reporter = LedgerReporter(ledger.frame())\n",
    "\n",
    "print(\"=== Ledger Summary ===\")\n",
    "display(reporter.counts())\n",
    "\n",
    "print(\"\\n=== Multiple Statistics Problem Demo ===\")\n",
    "print(\"Current approach using tags to distinguish statistics:\")\n",
    "\n",
    "# Show the ambiguity problem\n",
    "stats_events = (\n",
    "    ledger.frame()\n",
    "    .filter(ledger.frame()[\"namespace\"] == \"stats\")\n",
    "    .select([\"time_index\", \"tag\", \"payload_type\"])\n",
    "    .sort(\"time_index\")\n",
    ")\n",
    "\n",
    "print(f\"\\nStatistics events in 'stats' namespace:\")\n",
    "display(stats_events)\n",
    "\n",
    "print(\"\\nâŒ Problem: ledger.latest(namespace='stats') is ambiguous!\")\n",
    "print(\"   It could return CTR WaldZ, LoadTime Safe, or Bounce Safe statistic\")\n",
    "print(\"\\nâœ… Current workaround: ledger.latest(namespace='stats', tag='stat:ctr_waldz')\")\n",
    "print(\"   But this makes 'tag' feel mandatory rather than optional\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Proposed solutions:\")\n",
    "print(\"   1. Add 'metric_id' field: 'ctr', 'loadtime', 'bounce'\")\n",
    "print(\"   2. Hierarchical 'statistic_type': 'stats.ctr.waldz', 'stats.loadtime.safe'\")\n",
    "print(\"   3. Enhanced 'tag' system with required structure\")\n",
    "print(\"   4. Separate namespaces: 'stats:ctr', 'stats:loadtime', 'stats:bounce'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c236dd",
   "metadata": {},
   "source": [
    "## Visualization: Multi-Metric Dashboard\n",
    "\n",
    "Create a dashboard showing all metrics and their signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9833cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive dashboard\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle(\n",
    "    f\"Multi-Metric A/B Test Dashboard: {exp_id}\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "\n",
    "# Extract daily progression\n",
    "days = [r[\"day\"] for r in daily_results]\n",
    "sample_sizes = [r[\"total_n\"] for r in daily_results]\n",
    "ctr_a_rates = [r[\"ctr_a_rate\"] for r in daily_results]\n",
    "ctr_b_rates = [r[\"ctr_b_rate\"] for r in daily_results]\n",
    "\n",
    "# Plot 1: Sample size progression\n",
    "axes[0, 0].plot(days, sample_sizes, \"o-\", color=\"blue\", alpha=0.7)\n",
    "axes[0, 0].axhline(\n",
    "    y=experiment.target_n_per_arm * 2,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.5,\n",
    "    label=\"Target\",\n",
    ")\n",
    "axes[0, 0].set_title(\"Sample Size Over Time\")\n",
    "axes[0, 0].set_xlabel(\"Day\")\n",
    "axes[0, 0].set_ylabel(\"Total Sample Size\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: CTR progression\n",
    "axes[0, 1].plot(days, ctr_a_rates, \"o-\", label=\"A (Baseline)\", color=\"orange\")\n",
    "axes[0, 1].plot(days, ctr_b_rates, \"o-\", label=\"B (Variant)\", color=\"green\")\n",
    "axes[0, 1].set_title(\"Click-Through Rate Progression\")\n",
    "axes[0, 1].set_xlabel(\"Day\")\n",
    "axes[0, 1].set_ylabel(\"CTR\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Effect size progression\n",
    "ctr_effects = [b - a for a, b in zip(ctr_a_rates, ctr_b_rates)]\n",
    "axes[1, 0].plot(days, ctr_effects, \"o-\", color=\"purple\", alpha=0.8)\n",
    "axes[1, 0].axhline(y=0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "axes[1, 0].set_title(\"CTR Effect Size (B - A)\")\n",
    "axes[1, 0].set_xlabel(\"Day\")\n",
    "axes[1, 0].set_ylabel(\"Effect Size\")\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Information time adaptation\n",
    "planned_info_times = experiment.adaptive_info_time.planned_fractions\n",
    "actual_looks = min(look_counter, len(planned_info_times))\n",
    "if actual_looks > 0:\n",
    "    look_indices = list(range(1, actual_looks + 1))\n",
    "    axes[1, 1].plot(\n",
    "        look_indices,\n",
    "        planned_info_times[:actual_looks],\n",
    "        \"o--\",\n",
    "        label=\"Planned\",\n",
    "        color=\"blue\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    # For demo, show some adaptive adjustments\n",
    "    adapted_fractions = []\n",
    "    for i in range(actual_looks):\n",
    "        day_idx = min(\n",
    "            i * 3 + 2, len(sample_sizes) - 1\n",
    "        )  # Approximate sample size at look\n",
    "        adapted_t = experiment.adaptive_info_time.get_info_fraction(\n",
    "            i + 1, sample_sizes[day_idx] // 2\n",
    "        )\n",
    "        adapted_fractions.append(adapted_t)\n",
    "\n",
    "    axes[1, 1].plot(\n",
    "        look_indices, adapted_fractions, \"o-\", label=\"Adapted\", color=\"red\", alpha=0.8\n",
    "    )\n",
    "\n",
    "    axes[1, 1].set_title(\"Information Time: Planned vs Adapted\")\n",
    "    axes[1, 1].set_xlabel(\"Look Number\")\n",
    "    axes[1, 1].set_ylabel(\"Information Fraction\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        \"No looks completed\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        transform=axes[1, 1].transAxes,\n",
    "    )\n",
    "    axes[1, 1].set_title(\"Information Time Adaptation\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nðŸ“ˆ === Final Results Summary ===\")\n",
    "final_data = cumulative_data\n",
    "for metric_name, data in final_data.items():\n",
    "    if data[\"a_total\"] > 0 and data[\"b_total\"] > 0:\n",
    "        a_rate = data[\"a_success\"] / data[\"a_total\"]\n",
    "        b_rate = data[\"b_success\"] / data[\"b_total\"]\n",
    "        effect = b_rate - a_rate\n",
    "        rel_effect = (effect / a_rate) * 100 if a_rate > 0 else 0\n",
    "        print(\n",
    "            f\"{metric_name.upper():>10s}: A={a_rate:.4f}, B={b_rate:.4f}, \"\n",
    "            f\"Effect={effect:+.4f} ({rel_effect:+.1f}%)\"\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"\\nTotal sample size: {final_data['ctr']['a_total'] + final_data['ctr']['b_total']}\"\n",
    ")\n",
    "print(f\"Experiment duration: {experiment_days} days\")\n",
    "print(f\"Analysis looks: {look_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a2828",
   "metadata": {},
   "source": [
    "## Key Insights and Design Recommendations\n",
    "\n",
    "### 1. Multiple Statistics Namespace Problem\n",
    "\n",
    "**Issue**: When multiple statistics exist in the same experiment (CTR WaldZ, LoadTime Safe Test, Bounce Safe Test), they all use `namespace='stats'`, making `ledger.latest(namespace='stats')` ambiguous.\n",
    "\n",
    "**Current Workaround**: Use `tag` parameter: `ledger.latest(namespace='stats', tag='stat:ctr_waldz')`\n",
    "\n",
    "**Proposed Solutions**:\n",
    "\n",
    "1. **Add `metric_id` field**: \n",
    "   ```python\n",
    "   ledger.latest(namespace='stats', metric_id='ctr')\n",
    "   ```\n",
    "\n",
    "2. **Hierarchical `statistic_type`**:\n",
    "   ```python\n",
    "   statistic_type='stats.ctr.waldz'\n",
    "   statistic_type='stats.loadtime.safe_test'\n",
    "   ```\n",
    "\n",
    "3. **Structured namespace**:\n",
    "   ```python\n",
    "   namespace='stats:ctr', namespace='stats:loadtime'\n",
    "   ```\n",
    "\n",
    "### 2. Information Time Adaptation Benefits\n",
    "\n",
    "- **Flexibility**: Accounts for real-world sample size variability\n",
    "- **Wall-clock alignment**: Decisions based on calendar time, not sample milestones\n",
    "- **Maintains Type I error**: Proper statistical properties preserved\n",
    "\n",
    "### 3. Multi-Testing Framework\n",
    "\n",
    "- **Primary endpoint**: Full Î±=0.05 for business-critical CTR improvement\n",
    "- **Guardrails**: Bonferroni allocation for safety monitoring\n",
    "- **Independent monitoring**: Safe testing for continuous guardrail surveillance\n",
    "\n",
    "### 4. Operational Advantages\n",
    "\n",
    "- **Audit trail**: Complete event history for regulatory compliance\n",
    "- **Component separation**: Clear responsibility boundaries\n",
    "- **Extensibility**: Easy to add new metrics or modify criteria\n",
    "- **Real-time monitoring**: Daily data ingestion with scheduled analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
